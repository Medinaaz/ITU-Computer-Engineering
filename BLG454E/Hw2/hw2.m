%% @Author% Team Name: Bee Meister% Student Name: Emre ?zdil, Merve Ecevit% Student ID : 150120138, 150140115% Date: 30/04/2017%% clearing datasclear; close all;clc;% loading datas from fileload('data_logistic.mat');% getting the size of data[row,column] = size(z); % creating classes for Training and Test SettrainingClass0 = zeros(row,column);trainingClass1 = zeros(row,column);testClass0 = zeros(row,column);testClass1 = zeros(row,column);% the number of elements in both Training and Test Set classestrainingNumberClass0 = 1;trainingNumberClass1 = 1;testNumberClass0=1;testNumberClass1=1;% data between 1-75 and 91-165 -> trainingSet (150 row)% data between 76-90 and 166-180 -> testSet (30 row)trainingSet = z(1:75,1:3);trainingSet(76:150,1:3) = z(91:165,1:3);testSet = z(76:90,1:3);testSet(16:30,1:3) = z(166:180,1:3);% dividing the data of trainingSet into two classes which are class0 and class1% trainingNumberClass0 -> the number of Class 0 elements which are in trainingSet% trainingNumberClass1 -> the number of Class 1 elements which are in trainingSetfor i=1:150    if trainingSet(i,3) == 0      trainingClass0(trainingNumberClass0,1:3) = trainingSet(i,1:3);      trainingNumberClass0 = trainingNumberClass0 + 1;     else      trainingClass1(trainingNumberClass1,1:3) = trainingSet(i,1:3);      trainingNumberClass1 = trainingNumberClass1 + 1;    endend% figure of Training Set without decision boundary with class 0 and class 1figure('Name','Training Set - Class 0 and Class 1');plot(trainingClass0(1:trainingNumberClass0-1,1),trainingClass0(1:trainingNumberClass0-1,2),'b.');hold on;plot(trainingClass1(1:trainingNumberClass1-1,1),trainingClass1(1:trainingNumberClass1-1,2),'r.');xlabel('X1');ylabel('X2');legend('Class 0','Class 1');% splitting x1, x2 (x) and class label(y) of training Setx = trainingSet(1:150,1:2);y = trainingSet(1:150,3); % adding one more column to x which is full of 1's for classes[row2,column2] = size(x);x = [ones(row2,1) x];% creating theta variable for y = Q0 + x1*Q1 + x2*Q2,% according to the number of features and class labeltheta = zeros(column,1);% sigmoid functionXTh = x * theta; h = 1 ./ (1 + exp(-XTh));% cost functionJ = -(1/row) * sum((y .* log(h)) + ((1 - y) .* log(1 - h)));% alpha in the formulalearningRate = 1; % gradient descent Qi = Qi - J(Q)' % temp = J(Q)'for j=1:8000  for i = 1 : size(theta, 1)    temp = (1 / row) * sum((h - y) .* x(:, i));      theta(i) =  theta(i) - (learningRate * temp);  end  XTh = x * theta;   h = 1 ./ (1 + exp(-XTh));  end% getting the x1 and x2 (features) of Test Setx = testSet(1:30,1:2);[row1,column1] = size(x);% adding one more column to x which is full of 1's for classesx = [ones(row1,1) x]; % sigmoid will be used to determine the class of dataXTh = x * theta; h = 1 ./ (1 + exp(-XTh));% determine the classes of Test Set % If h>= 0.5 -> class 0% Else -> class 1for i=1:30    if h(i) < 0.5;       testSet(i,3) = 0;    else      testSet(i,3) = 1;    endend% comparing the label of classes to find the accuracyaccuracyTestSet = z(76:90,1:3);accuracyTestSet(16:30,1:3) = z(166:180,1:3);accuracy = 0;for i=1:30    if accuracyTestSet(i,3) == testSet(i,3)      accuracy = accuracy +1;    endendaccuracy = (accuracy / 30) * 100;message = sprintf('Accuracy of Logistic Regression = %f%%\n', accuracy) ;disp(message);% dividing the data of testSet into two classes which are class0 and class1% testNumberClass0 -> the number of Class 0 elements which are in testSet% testNumberClass1 -> the number of Class 1 elements which are in testSetfor i=1:30    if testSet(i,3) == 0      testClass0(testNumberClass0,1:3) = testSet(i,1:3);      testNumberClass0 = testNumberClass0 + 1;     else      testClass1(testNumberClass1,1:3) = testSet(i,1:3);      testNumberClass1 = testNumberClass1 + 1;    endend% figure of class 0 and class 1 of Test and Training Set with decision boundary   x1=0:0.01:0.1;x2= -(theta(1)+(theta(2))*x1)/theta(3);figure('Name','Test and Training Set - Class 0 and Class 1 with Decision Boundary');plot(x1,x2);hold on;plot(trainingClass0(1:trainingNumberClass0-1,1),trainingClass0(1:trainingNumberClass0-1,2),'y.');hold on;plot(trainingClass1(1:trainingNumberClass1-1,1),trainingClass1(1:trainingNumberClass1-1,2),'m.');hold on;plot(testClass0(1:testNumberClass0-1,1),testClass0(1:testNumberClass0-1,2),'b.');hold on;plot(testClass1(1:testNumberClass1-1,1),testClass1(1:testNumberClass1-1,2),'r.');xlabel('X1');ylabel('X2');legend('Decision Boundary','Training:Class 0','Training:Class 1','Test:Class 0','Test:Class 1');% figure of class 0 and class 1 of Training Set with decision boundary figure('Name','Training Set - Class 0 and Class 1 with Decision Boundary');plot(x1,x2);hold on;plot(trainingClass0(1:trainingNumberClass0-1,1),trainingClass0(1:trainingNumberClass0-1,2),'b.');hold on;plot(trainingClass1(1:trainingNumberClass1-1,1),trainingClass1(1:trainingNumberClass1-1,2),'r.');xlabel('X1');ylabel('X2');legend('Decision Boundary','Class 0','Class 1');% figure of class 0 and class 1 of Test Set with decision boundary figure('Name','Test Set - Class 0 and Class 1 with Decision Boundary');plot(x1,x2);hold on;plot(testClass0(1:testNumberClass0-1,1),testClass0(1:testNumberClass0-1,2),'b.');hold on;plot(testClass1(1:testNumberClass1-1,1),testClass1(1:testNumberClass1-1,2),'r.');xlabel('X1');ylabel('X2');legend('Decision Boundary','Class 0','Class 1');